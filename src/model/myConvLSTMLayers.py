import tensorflow.compat.v1 as tf
from textwrap import wrap
import matplotlib as plt
import numpy as np
from sklearn.metrics import confusion_matrix
from timeit import default_timer as timer
import src.config as cfg


class myConvLSTMLayers:
    def __init__(self): 
        print("Initializing Layers")
        
    def conv2d(self,x, W, b, stridex,stridey,name):
        # Conv2D wrapper, with bias and relu activation
        with tf.name_scope("Convolutional"): 
            x = tf.nn.conv2d(x, W, strides=[1, stridex, stridey, 1], padding='SAME',name=name)
            x = tf.nn.bias_add(x, b)
            return tf.nn.relu(x)     
        
    def conv1d(self,x, W, b, strides,name):
        # Conv2D wrapper, with bias and relu activation
        with tf.name_scope("Convolutional"):
            x = tf.nn.conv1d(x, W, stride=strides, padding='SAME',name=name)
            x = tf.nn.bias_add(x, b)
            return tf.nn.relu(x)     


    def maxpool2d(self,x, k=2,name=None):
        # MaxPool2D wrapper
        return tf.nn.max_pool(x, ksize=[1, k, k, 1], strides=[1, k, k, 1],
        padding='SAME')    
        
    def LSTMLayer(self, x, name ="LSTM-Cells"):
        num_unitsLSTM = cfg.NN_LSTM['num_unitsLSTM']
        dropout = cfg.Modelparameter_highlevel['dropout']
        lstm_keep_prob = 1-dropout
        #inputs war self.conv6flat
        with tf.name_scope(name):
            lstm_cells = [tf.nn.rnn_cell.LSTMCell(num_units=num_unitsLSTM, activation=tf.nn.relu)
                          for layer in range(cfg.NN_LSTM['n_layersLSTM'])]
            cells_drop = [tf.nn.rnn_cell.DropoutWrapper(cell,input_keep_prob=lstm_keep_prob)
                          for cell in lstm_cells ]
        multi_cell = tf.nn.rnn_cell.MultiRNNCell(cells_drop)
            
        outputs, states = tf.nn.dynamic_rnn(multi_cell, x, dtype=tf.float32)
        top_layer_h_state = states[-1][1]  
        return top_layer_h_state
       
    def fcLayer(self, x, W, b, training):       
        fc1 = tf.add(tf.matmul(x, W), b)
        fc1 = tf.nn.relu(fc1)        
        fc1 = tf.layers.dropout(fc1, rate = cfg.Modelparameter_highlevel['dropout'], training = training)
        return fc1  
