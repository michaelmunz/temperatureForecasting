from textwrap import wrap
import re
import math
import itertools
#import tfplot # import tensorflow-plot
import matplotlib as plt
import numpy as np
from sklearn.metrics import confusion_matrix
from timeit import default_timer as timer
import src.config as cfg
import src.model.myConvLSTMLayers as layers

import tensorflow.compat.v1 as tf
import tensorflow as tf2
tf.disable_v2_behavior()

import pickle


class myConvLSTM:
    def __init__(self,samples_train,samples_val, samples_test, samplelabels_train,samplelabels_val, samplelabels_test, sampleinfo_train, sampleinfo_val, sampleinfo_test): 
    ############### GET LAYERS ##########################
    
        samples_train = np.array(samples_train, dtype = np.float32)
        samples_val = np.array(samples_val, dtype = np.float32)    
        samples_test = np.array(samples_test, dtype = np.float32)       
        samplelabels_train = np.array(samplelabels_train, dtype = np.float32)    
        samplelabels_val = np.array(samplelabels_val, dtype = np.float32)    
        samplelabels_test = np.array(samplelabels_test, dtype = np.float32)
        sampleinfo_train = np.array(sampleinfo_train, dtype = np.int32)    
        sampleinfo_val = np.array(sampleinfo_val, dtype = np.int32)    
        sampleinfo_test = np.array(sampleinfo_test, dtype = np.int32) 
        
        sampleinfo_train = np.reshape(sampleinfo_train,(sampleinfo_train.shape[0]*sampleinfo_train.shape[1],1))
        sampleinfo_val = np.reshape(sampleinfo_val,(sampleinfo_val.shape[0]*sampleinfo_val.shape[1],1))
        sampleinfo_test = np.reshape(sampleinfo_test,(sampleinfo_test.shape[0]*sampleinfo_test.shape[1],1))
        samples_train = np.reshape(samples_train,(samples_train.shape[0]*samples_train.shape[1],samples_train.shape[2],samples_train.shape[3]))
        samples_val = np.reshape(samples_val,(samples_val.shape[0]*samples_val.shape[1],samples_val.shape[2],samples_val.shape[3]))
        samples_test = np.reshape(samples_test,(samples_test.shape[0]*samples_test.shape[1],samples_test.shape[2],samples_test.shape[3]))
        samplelabels_train = np.reshape(samplelabels_train,(samplelabels_train.shape[0]*samplelabels_train.shape[1],samplelabels_train.shape[2]))
        samplelabels_val = np.reshape(samplelabels_val,(samplelabels_val.shape[0]*samplelabels_val.shape[1],samplelabels_val.shape[2]))
        samplelabels_test = np.reshape(samplelabels_test,(samplelabels_test.shape[0]*samplelabels_test.shape[1],samplelabels_test.shape[2]))   
        
        
        
        
        self.samples_train = samples_train
        self.samples_val=samples_val
        self.samples_test=samples_test
        self.samplelabels_train=samplelabels_train
        self.samplelabels_val=samplelabels_val
        self.samplelabels_test=samplelabels_test
        self.sampleinfo_train = sampleinfo_train
        self.sampleinfo_val = sampleinfo_val
        self.sampleinfo_test = sampleinfo_test
        
        self.myLayers =layers.myConvLSTMLayers()
        self.training = tf.placeholder_with_default(False,shape=(),name="training")    
        
        batchsize =  cfg.Modelparameter_highlevel['batchsize']
        self.num_samples_train =  samples_train.shape[0]/batchsize
        self.num_samples_val =   samples_val.shape[0]/batchsize
        self.num_samples_test =   samples_test.shape[0]/batchsize
        
        self.samples_train_placeholder = tf.placeholder(tf.float32, samples_train.shape)
        self.samplelabels_train_placeholder = tf.placeholder(tf.float32, samplelabels_train.shape)
        self.sampleinfo_train_placeholder = tf.placeholder(tf.int32, sampleinfo_train.shape)

    
        #%% Generate datasets for the validation set. Validation set doesn´t need 
        ## placeholder, as the amount of images never changes with them
        train_dataset = tf.data.Dataset.from_tensor_slices((self.samples_train_placeholder,self.samplelabels_train_placeholder, self.sampleinfo_train_placeholder))
            ## work with this dataset+
            
        train_dataset = train_dataset.map(self.parse_function, num_parallel_calls=4)        # get the batch size
        train_dataset = train_dataset.batch(batchsize, drop_remainder=True).shuffle(buffer_size = int(self.num_samples_train))
        #train_dataset = train_dataset.apply(tf.data.experimental.shuffle_and_repeat(7000, 1))
       # train_dataset = train_dataset
        # Prefetch one batch to make sure that a batch is ready to be served at all time
        train_dataset = train_dataset.prefetch(1)        
        iterator = tf.data.Iterator.from_structure(train_dataset.output_types, train_dataset.output_shapes)
        
        # Initializer for the validation dataset
        self.train_init_op = iterator.make_initializer(train_dataset)

        
        self.samples_test_placeholder = tf.placeholder(tf.float32, samples_test.shape)
        self.samplelabels_test_placeholder = tf.placeholder(tf.float32, samplelabels_test.shape)
        self.sampleinfo_test_placeholder = tf.placeholder(tf.int32, sampleinfo_test.shape)

        #%% Generate datasets for the validation set. Validation set doesn´t need 
        ## placeholder, as the amount of images never changes with them
        test_dataset = tf.data.Dataset.from_tensor_slices((self.samples_test_placeholder,self.samplelabels_test_placeholder,self.sampleinfo_test_placeholder))
        test_dataset = test_dataset.map(self.parse_function, num_parallel_calls=4)
        # get the batch size
        test_dataset = test_dataset.batch(batchsize, drop_remainder=True)
        # Prefetch one batch to make sure that a batch is ready to be served at all time
        test_dataset = test_dataset.prefetch(1) 
        
 
        
        # Initializer for the validation dataset
        self.test_init_op = iterator.make_initializer(test_dataset)
    
    
    
        self.samples_val_placeholder = tf.placeholder(tf.float32, samples_val.shape)
        self.samplelabels_val_placeholder = tf.placeholder(tf.float32, samplelabels_val.shape) 
        self.sampleinfo_val_placeholder = tf.placeholder(tf.int32, sampleinfo_val.shape)

       
        #%%  Generate datasets for the validation set. Validation set doesn´t need 
        ## placeholder, as the amount of images never changes with them
        val_dataset = tf.data.Dataset.from_tensor_slices(( self.samples_val_placeholder,self.samplelabels_val_placeholder, self.sampleinfo_val_placeholder))
        val_dataset = val_dataset.map(self.parse_function, num_parallel_calls=4)
        # get the batch size
        val_dataset = val_dataset.batch(batchsize, drop_remainder=True)
        # Prefetch one batch to make sure that a batch is ready to be served at all time
        val_dataset = val_dataset.prefetch(1)        
        
        
        
        # Initializer for the validation dataset
        self.val_init_op = iterator.make_initializer(val_dataset)
        ### Generate batch from the individual datasets:
        self.next_element = iterator.get_next()
        
        
    def parse_function(self,samples,label, info):
        samples = tf.reshape(samples, shape=[samples.shape[0],samples.shape[1],cfg.NN_input['channels']]) 
       # label = tf.reshape(label, shape=[label.shape[0]]) 
        return samples, label, info
    
    
    ###############################################################################
    ##################### Built graph ############################################# 
    ###############################################################################  

    def built_graph(self):
        with tf.name_scope("inputs"):   
#            self.X = tf.placeholder(tf.float32, shape=[None,cfg.NN_input['height'],cfg.NN_input['width']], name="X")
#            self.X_reshaped = tf.reshape( self.X, shape=[-1, cfg.NN_input['height'], cfg.NN_input['width'], cfg.NN_input['channels']])     
#            self.y = tf.placeholder(tf.int32, shape=[None,cfg.NN_Dense['n_outputs']], name="y")  
            initializer = tf2.initializers.GlorotUniform()
            a = math.ceil(cfg.NN_input['height']/4) * math.ceil(cfg.NN_input['width']/4) *cfg.NN_CNN['conv6_fmaps']
            ############### INITIALIZING WEIGHTS ################
            self.weights = {
                        # 5x5 conv, 1 input, 32 outputs
                        'wc1': tf.Variable(initializer([cfg.NN_CNN['conv1_ksizex'], cfg.NN_CNN['conv1_ksizey'], 1, cfg.NN_CNN['conv1_fmaps']])),
                        # 5x5 conv, 32 inputs, 64 outputs
                        'wc2': tf.Variable(initializer([cfg.NN_CNN['conv2_ksizex'], cfg.NN_CNN['conv2_ksizey'], cfg.NN_CNN['conv1_fmaps'], cfg.NN_CNN['conv2_fmaps']])),            
                         # 5x5 conv, 32 inputs, 64 outputs
                        'wc3': tf.Variable(initializer([cfg.NN_CNN['conv3_ksizex'], cfg.NN_CNN['conv3_ksizey'], cfg.NN_CNN['conv2_fmaps'], cfg.NN_CNN['conv3_fmaps']])),           
                         # 5x5 conv, 32 inputs, 64 outputs
                        'wc4': tf.Variable(initializer([cfg.NN_CNN['conv4_ksizex'], cfg.NN_CNN['conv4_ksizey'], cfg.NN_CNN['conv3_fmaps'], cfg.NN_CNN['conv4_fmaps']])),   
                         # 5x5 conv, 32 inputs, 64 outputs
                        'wc5': tf.Variable(initializer([cfg.NN_CNN['conv5_ksizex'], cfg.NN_CNN['conv5_ksizey'], cfg.NN_CNN['conv4_fmaps'], cfg.NN_CNN['conv5_fmaps']])),           
                         # 5x5 conv, 32 inputs, 64 outputs
                        'wc6': tf.Variable(initializer([cfg.NN_CNN['conv6_ksizex'], cfg.NN_CNN['conv6_ksizey'], cfg.NN_CNN['conv5_fmaps'], cfg.NN_CNN['conv6_fmaps']])), 
                        
                        # LSTM
                        'wlstm1': tf.Variable(tf.random_normal([cfg.NN_LSTM['num_unitsLSTM'],cfg.NN_LSTM['num_unitsLSTM']])),
                        # fully connected, 7*7*64 inputs, 1024 outputs
                        'wd1': tf.Variable(tf.random_normal([cfg.NN_LSTM['num_unitsLSTM'], cfg.NN_Dense['n_fc1']])),                                             
                        # 1024 inputs, 10 outputs (class prediction)
                        'wd2': tf.Variable(tf.random_normal([cfg.NN_Dense['n_fc1'], cfg.NN_Dense['n_fcout']])),
                        # 1024 inputs, 10 outputs (class prediction)
                        'out': tf.Variable(tf.random_normal([cfg.NN_Dense['n_fcout'], cfg.NN_Dense['n_outputs']]))
                        }                         
            self.biases = {
                        'bc1': tf.Variable(initializer([cfg.NN_CNN['conv1_fmaps']])),
                        'bc2': tf.Variable(initializer([cfg.NN_CNN['conv2_fmaps']])),
                        'bc3': tf.Variable(initializer([cfg.NN_CNN['conv3_fmaps']])),
                        'bc4': tf.Variable(initializer([cfg.NN_CNN['conv4_fmaps']])),
                        'bc5': tf.Variable(initializer([cfg.NN_CNN['conv5_fmaps']])),
                        'bc6': tf.Variable(initializer([cfg.NN_CNN['conv6_fmaps']])),
                        'blstm1':tf.Variable(tf.random_normal([cfg.NN_LSTM['num_unitsLSTM']])),
                        'bd1': tf.Variable(tf.random_normal([cfg.NN_Dense['n_fc1']])),
                        'bd2': tf.Variable(tf.random_normal([cfg.NN_Dense['n_fcout']])),
                        'out': tf.Variable(tf.random_normal([cfg.NN_Dense['n_outputs']]))
                        } 
             
        with tf.name_scope("Conv2D"):  
            # Convolution Layer
            conv1 =  self.myLayers.conv2d(self.next_element[0], self.weights['wc1'], self.biases['bc1'],1,1,"conv1")             
            conv2 =  self.myLayers.conv2d(conv1, self.weights['wc2'], self.biases['bc2'],1,1,"conv2")            
            # Max Pooling (down-sampling)
            conv2 =  self.myLayers.maxpool2d(conv2, k=2,name="maxpool1")             
            # Convolution Layer
            conv3 =  self.myLayers.conv2d(conv2, self.weights['wc3'], self.biases['bc3'],1,1,"conv3")
            conv4 =  self.myLayers.conv2d(conv3, self.weights['wc4'], self.biases['bc4'],1,1,"conv4")
            # Max Pooling (down-sampling)
            conv4 =  self.myLayers.maxpool2d(conv4, k=2,name="maxpool2")            
#              # Convolution Layer
            conv5 =  self.myLayers.conv2d(conv4, self.weights['wc5'], self.biases['bc5'],1,1,"conv5")
            self.conv6 =  self.myLayers.conv2d(conv5, self.weights['wc6'], self.biases['bc6'],1,1,"conv6")
            # Max Pooling (down-sampling)
            self.conv6 =  self.myLayers.maxpool2d(self.conv6, k=2,name="maxpool2")
            self.conv6flat = tf.reshape(self.conv6, shape=[-1, 1,math.ceil(cfg.NN_input['height']/6)*math.ceil(cfg.NN_input['width']/8)*cfg.NN_CNN['conv6_fmaps']]) #/8 weil 3 mal maxpool mit k=2, also : /2/2/2
       
        with tf.name_scope("LSTM"):    
             top_layer_h_state =  self.myLayers.LSTMLayer(self.conv6flat)
             
        with tf.name_scope("fc"):
            fc1 =  self.myLayers.fcLayer(top_layer_h_state,self.weights['wd1'],self.biases['bd1'], self.training)
#            fce =  self.myLayers.fcLayer(fc1,self.weights['wde'],self.biases['bde'], self.training)
            self.fc2 =  self.myLayers.fcLayer(fc1,self.weights['wd2'],self.biases['bd2'], self.training)                  
        
        with tf.name_scope("output"):
            self.OUTPUT = tf.add(tf.matmul(self.fc2, self.weights['out']), self.biases['out'])
#            self.OUTPUT = tf.layers.dense(self.fc2, cfg.NN_Dense['n_outputs'], name="output")
#            self.Y_proba = tf.nn.softmax(self.logits, name="Y_proba")  #if classification
        
        with tf.name_scope("train"):
#            self.MSE=  tf.losses.absolute_difference(labels = self.next_element[1], predictions = self.OUTPUT) # testL1 
            self.MSE = tf.square(self.next_element[1] - self.OUTPUT)
            regularizers = (cfg.Modelparameter_highlevel['l2_reg_param']*tf.nn.l2_loss(self.weights['wc1']) + 
                            cfg.Modelparameter_highlevel['l2_reg_param']*tf.nn.l2_loss(self.weights['wc2']) + 
                            cfg.Modelparameter_highlevel['l2_reg_param']*tf.nn.l2_loss(self.weights['wc3']) + 
                            cfg.Modelparameter_highlevel['l2_reg_param']*tf.nn.l2_loss(self.weights['wc4']) +
                            cfg.Modelparameter_highlevel['l2_reg_param']*tf.nn.l2_loss(self.weights['wd1']) +
                            cfg.Modelparameter_highlevel['l2_reg_param']*tf.nn.l2_loss(self.weights['wd2']) + 
                            cfg.Modelparameter_highlevel['l2_reg_param']*tf.nn.l2_loss(self.weights['wc5']) + 
                            cfg.Modelparameter_highlevel['l2_reg_param']*tf.nn.l2_loss(self.weights['wc6']))
            self.loss = tf.reduce_mean(self.MSE + regularizers)
            
            #global_step = tf.Variable(0, trainable=False)
            #decayed_lr = tf.train.exponential_decay(learning_rate=cfg.Modelparameter_highlevel['learningrate'], global_step, 10000, 0.95, staircase=True)
            optimizer = tf.train.AdamOptimizer(
                    learning_rate=cfg.Modelparameter_highlevel['learningrate']
                    )
            self.training_op = optimizer.minimize(self.loss)
            
            
      
            
            
            
                  
         ######## TENSORBOARD ########################           
        with tf.name_scope("init_and_save"):
            self.init = tf.global_variables_initializer()
            tf.add_to_collection('loss', self.loss)            
            tf.add_to_collection('X', self.next_element[0])
            tf.add_to_collection('y', self.next_element[1])                    
            tf.add_to_collection('pred', self.OUTPUT)            
            tf.add_to_collection('conv6', self.conv6)
            tf.add_to_collection('wconv6', self.weights['wc6'])   
            tf.add_to_collection('conv5', conv5)
            tf.add_to_collection('wconv5', self.weights['wc5'])   
            tf.add_to_collection('conv4', conv4)
            tf.add_to_collection('wconv4', self.weights['wc4'])   
            tf.add_to_collection('conv3', conv3)
            tf.add_to_collection('wconv3', self.weights['wc3'])   
            tf.add_to_collection('conv2', conv2)
            tf.add_to_collection('wconv2', self.weights['wc2'])   
            tf.add_to_collection('conv1', conv1)
            tf.add_to_collection('wconv1', self.weights['wc1'])
            tf.add_to_collection('fc1', fc1)
            tf.add_to_collection('wfc1', self.weights['wd1'])   
            tf.add_to_collection('fc2', self.fc2)
            tf.add_to_collection('wfc2', self.weights['wd2'])
            tf.add_to_collection('wout', self.weights['out'])

            self.saver = tf.train.Saver()      
         

            
    ###############################################################################
    ##################### RUN IT ################################################## 
    ###############################################################################          
            
    def runGraph(self):      

       
        with tf.Session() as self.sess:
            self.init.run()
            MSElist_val = list()
            MSElist_train = list()
            for epoch in range(cfg.Modelparameter_highlevel['n_epochs']):
                self.sess.run(self.train_init_op,feed_dict={self.samples_train_placeholder : self.samples_train, self.samplelabels_train_placeholder : self.samplelabels_train,self.sampleinfo_train_placeholder: self.sampleinfo_train, self.training : True})
                MSEtrain = 0
                MSEval = 0
                pred_train_list = list()
                pred_val_list   = list()
                true_train_list = list()
                true_val_list   = list()
                conv6_train_list = list()
                conv6_val_list = list()
                x_train_list = list()
                x_val_list = list()
                sampleinfo_train_list = list()
                sampleinfo_val_list = list()
                
                
                
                
                start = timer()
                ## Iterate through the dataset until no sample in the directories event and normal is left.
                while True:
                    try:
                        _, train_loss, batch_X_train, batch_y_train , pred_train, conv6_train, W_d1, sampleinfo_train  =  self.sess.run([self.training_op, self.loss,self.next_element[0], self.next_element[1],self.OUTPUT, self.conv6, self.weights['wd1'],self.next_element[2]])  
                        MSEtrain += train_loss
                        pred_train_list.append(np.array(pred_train))
                        true_train_list.append(np.array(batch_y_train))
                        conv6_train_list.append(np.array(conv6_train))
                        x_train_list.append(np.array(batch_X_train))
                        sampleinfo_train_list.append(np.array(sampleinfo_train))

               
                    except tf.errors.OutOfRangeError:
                        break
                

                pred_train_list = np.concatenate([np.array(ll) for ll in pred_train_list])
                conv6_train_list = np.concatenate([np.array(ll) for ll in conv6_train_list])
                true_train_list = np.concatenate([np.array(ll) for ll in true_train_list])
                x_train_list = np.concatenate([np.array(ll) for ll in x_train_list])
                sampleinfo_train_list = np.concatenate([np.array(ll) for ll in sampleinfo_train_list])

                MSElist_train.append(MSEtrain/self.num_samples_train)   
                
                self.sess.run(self.val_init_op,feed_dict={self.samples_val_placeholder : self.samples_val, self.samplelabels_val_placeholder : self.samplelabels_val, self.sampleinfo_val_placeholder: self.sampleinfo_val, self.training : False})              
                ## Iterate through the dataset until no sample in the directories event and normal is left.
                while True:
                    try:
                        val_loss, batch_X_val, batch_y_val, pred_val,conv6_val, sampleinfo_val  =  self.sess.run([self.loss, self.next_element[0], self.next_element[1],self.OUTPUT,self.conv6,self.next_element[2]])  
                        MSEval += val_loss
                        pred_val_list.append(np.array(pred_val))
                        true_val_list.append(np.array(batch_y_val))
                        conv6_val_list.append(np.array(conv6_val))
                        x_val_list.append(np.array(batch_X_val))
                        sampleinfo_val_list.append(np.array(sampleinfo_val))

                    except tf.errors.OutOfRangeError:
                        break
                MSElist_val.append(MSEval/self.num_samples_val)                  

                conv6_val_list = np.concatenate([np.array(ll) for ll in conv6_val_list])
                pred_val_list = np.concatenate([np.array(ll) for ll in pred_val_list])
                true_val_list = np.concatenate([np.array(ll) for ll in true_val_list])
                x_val_list = np.concatenate([np.array(ll) for ll in x_val_list])
                sampleinfo_val_list = np.concatenate([np.array(ll) for ll in sampleinfo_val_list])

                end = timer()
                print(epoch, "Train loss:", MSEtrain/self.num_samples_train,"Validation loss:",MSEval/self.num_samples_val,"Time in sek:" ,(end-start))
                
                
                
                
                if epoch % 100 == 0:
                    with open('saver/outputs/Prediction_Train/pred_train_' + str(epoch) + '.pckl', 'wb') as outfile:
                        pickle.dump( [x_train_list, true_train_list,pred_train_list], outfile) 
                    with open('saver/outputs/Prediction_Validation/pred_val_' + str(epoch) + '.pckl', 'wb') as outfile:
                        pickle.dump( [x_val_list, true_val_list,pred_val_list], outfile) 
                    with open('saver/outputs/Prediction_Train/Activations/conv6_train_' + str(epoch) + '.pckl', 'wb') as outfile:
                        pickle.dump( conv6_train_list, outfile) 
                    with open('saver/outputs/Prediction_Validation/Activations/conv6_val_' + str(epoch) + '.pckl', 'wb') as outfile:
                        pickle.dump( conv6_val_list, outfile)                         
                    with open('saver/outputs/Prediction_Train/Activations/wd1_train_' + str(epoch) + '.pckl', 'wb') as outfile:
                        pickle.dump(W_d1, outfile) 
                    with open('saver/outputs/Prediction_Validation/Activations/wd1_val_' + str(epoch) + '.pckl', 'wb') as outfile:
                        pickle.dump(W_d1, outfile) 
                    with open('saver/outputs/Prediction_Train/extraData/info_train_' + str(epoch) + '.pckl', 'wb') as outfile:
                        pickle.dump(sampleinfo_train_list, outfile) 
                    with open('saver/outputs/Prediction_Validation/extraData/info_val_' + str(epoch) + '.pckl', 'wb') as outfile:
                        pickle.dump(sampleinfo_val_list, outfile) 


                    
                
            with open('saver/outputs/losses/losses.pckl', 'wb') as outfile:
                pickle.dump( [np.array(MSElist_train), np.array(MSElist_val)], outfile)      
#
#                if cfg.Tensorboard['savetoTensorboard']:
#                    self.addtoTensorboard(epoch,num_samples_train,iteration,X_batch,y_batch,X_batchtest,y_batchtest)              

            
            cfg.Tensorboard['saverpath'] = self.saver.save(self.sess, "saver/outputs/model/model")  
            print("Model saved in path: %s" % cfg.Tensorboard['saverpath'])
            
        #%%####### Predicting whole testset #################
            Pred = list()
            y = list()
            x_test_list = list()
            conv6_test_list = list()
            sampleinfo_test_list = list()
            
            MSEtest = 0
            self.sess.run(self.test_init_op,feed_dict={self.samples_test_placeholder : self.samples_test, self.samplelabels_test_placeholder : self.samplelabels_test, self.sampleinfo_test_placeholder: self.sampleinfo_test, self.training : False})   
       ## Iterate through the dataset until no sample in the directories event and normal is left.
            while True:
                try:
                    test_loss, batch_X_test, batch_y_test , pred_test, conv6_test, sampleinfo_test =  self.sess.run([ self.loss,self.next_element[0], self.next_element[1],self.OUTPUT,self.conv6,self.next_element[2]])  
                    MSEtest += test_loss                    
                    Pred.append(np.array(pred_test))
                    y.append(np.array(batch_y_test))
                    conv6_test_list.append(np.array(conv6_test))
                    x_test_list.append(np.array(batch_X_test))
                    sampleinfo_test_list.append(np.array(sampleinfo_test))
                except tf.errors.OutOfRangeError:
                    break


            conv6_test_list = np.concatenate([np.array(ll) for ll in conv6_test_list])
            Pred = np.concatenate([np.array(ll) for ll in Pred])
            y = np.concatenate([np.array(ll) for ll in y])
            x_test_list = np.concatenate([np.array(ll) for ll in x_test_list])
            sampleinfo_test_list = np.concatenate([np.array(ll) for ll in sampleinfo_test_list])


            with open('saver/outputs/Prediction_Test/pred_test.pckl', 'wb') as outfile:
                pickle.dump( [x_test_list, Pred,y], outfile)
            with open('saver/outputs/Prediction_Test/Activations/conv6_test_' + str(epoch) + '.pckl', 'wb') as outfile:
                pickle.dump( conv6_test_list, outfile)                         
            with open('saver/outputs/Prediction_Test/Activations/wd1_test_' + str(epoch) + '.pckl', 'wb') as outfile:
                pickle.dump( W_d1, outfile) 
            with open('saver/outputs/Prediction_Test/extraData/info_test_' + str(epoch) + '.pckl', 'wb') as outfile:
                pickle.dump( sampleinfo_test_list, outfile) 

                
        return Pred, y
